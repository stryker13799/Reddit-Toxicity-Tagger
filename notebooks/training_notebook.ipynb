{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59848</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59849</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59852</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59856</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                       comment_text  \\\n",
       "id                                                                   \n",
       "59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "59855  0.000000  Is this something I'll be able to install on m...   \n",
       "59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack   insult  threat  asian  \\\n",
       "id                                                                         \n",
       "59848         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59849         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59852         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59855         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59856         0.021277      0.0         0.021277  0.87234     0.0    0.0   \n",
       "\n",
       "       atheist  bisexual  ...  article_id    rating  funny  wow  sad  likes  \\\n",
       "id                        ...                                                 \n",
       "59848      NaN       NaN  ...        2006  rejected      0    0    0      0   \n",
       "59849      NaN       NaN  ...        2006  rejected      0    0    0      0   \n",
       "59852      NaN       NaN  ...        2006  rejected      0    0    0      0   \n",
       "59855      NaN       NaN  ...        2006  rejected      0    0    0      0   \n",
       "59856      0.0       0.0  ...        2006  rejected      0    0    0      1   \n",
       "\n",
       "       disagree  sexual_explicit  identity_annotator_count  \\\n",
       "id                                                           \n",
       "59848         0              0.0                         0   \n",
       "59849         0              0.0                         0   \n",
       "59852         0              0.0                         0   \n",
       "59855         0              0.0                         0   \n",
       "59856         0              0.0                         4   \n",
       "\n",
       "       toxicity_annotator_count  \n",
       "id                               \n",
       "59848                         4  \n",
       "59849                         4  \n",
       "59852                         4  \n",
       "59855                         4  \n",
       "59856                        47  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnns =  ['target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df = data[columnns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59848</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59849</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59852</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59856</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333967</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Maybe the tax on \"things\" would be collected w...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333969</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>What do you call people who STILL think the di...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333982</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>thank you ,,,right or wrong,,, i am following ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334009</th>\n",
       "      <td>0.621212</td>\n",
       "      <td>Anyone who is quoted as having the following e...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Students defined as EBD are legally just as di...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1804874 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           target                                       comment_text  \\\n",
       "id                                                                     \n",
       "59848    0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "59849    0.000000  Thank you!! This would make my life a lot less...   \n",
       "59852    0.000000  This is such an urgent design problem; kudos t...   \n",
       "59855    0.000000  Is this something I'll be able to install on m...   \n",
       "59856    0.893617               haha you guys are a bunch of losers.   \n",
       "...           ...                                                ...   \n",
       "6333967  0.000000  Maybe the tax on \"things\" would be collected w...   \n",
       "6333969  0.000000  What do you call people who STILL think the di...   \n",
       "6333982  0.000000  thank you ,,,right or wrong,,, i am following ...   \n",
       "6334009  0.621212  Anyone who is quoted as having the following e...   \n",
       "6334010  0.000000  Students defined as EBD are legally just as di...   \n",
       "\n",
       "         severe_toxicity   obscene  identity_attack    insult  threat  \n",
       "id                                                                     \n",
       "59848           0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "59849           0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "59852           0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "59855           0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "59856           0.021277  0.000000         0.021277  0.872340     0.0  \n",
       "...                  ...       ...              ...       ...     ...  \n",
       "6333967         0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "6333969         0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "6333982         0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "6334009         0.030303  0.030303         0.045455  0.621212     0.0  \n",
       "6334010         0.000000  0.000000         0.000000  0.000000     0.0  \n",
       "\n",
       "[1804874 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Symbols in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_similar = \"\"\"â€™'â€˜Ã†ÃÆŽÆÆÆ”Ä²ÅŠÅ’áºžÃžÇ·ÈœÃ¦Ã°ÇÉ™É›É£Ä³Å‹Å“Ä¸Å¿ÃŸÃ¾Æ¿ÈÄ„ÆÃ‡ÄÆŠÄ˜Ä¦Ä®Æ˜ÅÃ˜Æ Åž\n",
    "È˜Å¢ÈšÅ¦Å²Æ¯YÌ¨Æ³Ä…É“Ã§Ä‘É—Ä™Ä§Ä¯Æ™Å‚Ã¸Æ¡ÅŸÈ™Å£È›Å§Å³Æ°yÌ¨Æ´ÃÃ€Ã‚Ã„ÇÄ‚Ä€ÃƒÃ…ÇºÄ„Ã†Ç¼Ç¢ÆÄ†ÄŠÄˆÄŒÃ‡ÄŽá¸ŒÄÆŠÃÃ‰ÃˆÄ–ÃŠ\n",
    "Ã‹ÄšÄ”Ä’Ä˜áº¸ÆŽÆÆÄ ÄœÇ¦ÄžÄ¢Æ”Ã¡Ã Ã¢Ã¤ÇŽÄƒÄÃ£Ã¥Ç»Ä…Ã¦Ç½Ç£É“Ä‡Ä‹Ä‰ÄÃ§Äá¸Ä‘É—Ã°Ã©Ã¨Ä—ÃªÃ«Ä›Ä•Ä“Ä™áº¹ÇÉ™É›Ä¡ÄÇ§ÄŸÄ£\n",
    "É£Ä¤á¸¤Ä¦IÃÃŒÄ°ÃŽÃÇÄ¬ÄªÄ¨Ä®á»ŠÄ²Ä´Ä¶Æ˜Ä¹Ä»ÅÄ½Ä¿Ê¼NÅƒNÌˆÅ‡Ã‘Å…ÅŠÃ“Ã’Ã”Ã–Ç‘ÅŽÅŒÃ•Åá»ŒÃ˜Ç¾Æ Å’Ä¥á¸¥Ä§Ä±Ã­Ã¬iÃ®Ã¯ÇÄ­\n",
    "Ä«Ä©Ä¯á»‹Ä³ÄµÄ·Æ™Ä¸ÄºÄ¼Å‚Ä¾Å€Å‰Å„nÌˆÅˆÃ±Å†Å‹Ã³Ã²Ã´Ã¶Ç’ÅÅÃµÅ‘á»Ã¸Ç¿Æ¡Å“Å”Å˜Å–ÅšÅœÅ ÅžÈ˜á¹¢áºžÅ¤Å¢á¹¬Å¦ÃžÃšÃ™Ã›ÃœÇ“Å¬ÅªÅ¨\n",
    "Å°Å®Å²á»¤Æ¯áº‚áº€Å´áº„Ç·Ãá»²Å¶Å¸È²á»¸Æ³Å¹Å»Å½áº’Å•Å™Å—Å¿Å›ÅÅ¡ÅŸÈ™á¹£ÃŸÅ¥Å£á¹­Å§Ã¾ÃºÃ¹Ã»Ã¼Ç”Å­Å«Å©Å±Å¯Å³á»¥Æ°áºƒáºÅµáº…Æ¿Ã½á»³Å·Ã¿È³á»¹Æ´ÅºÅ¼Å¾áº“\"\"\"\n",
    "\n",
    "SYMBOLS_TO_DELETE = \"\"\"\n",
    "\\nðŸ•\\rðŸµðŸ˜‘\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±\n",
    "ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009\n",
    "ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾\n",
    "ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘\n",
    "ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°\n",
    "ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…\n",
    "ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡\n",
    "â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼\n",
    "ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´\n",
    "Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€\n",
    "ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\n",
    "\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦\n",
    "ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\n",
    "\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½\n",
    "ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Ž\n",
    "å…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸\n",
    "ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘\n",
    "å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆ\n",
    "ðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\n",
    "\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’\n",
    "ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆ\n",
    "ðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼\n",
    "\"\"\"\n",
    "\n",
    "SYMBOLS_TO_ISOLATE = \"\"\".,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\Â°<~\\xa0â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\n",
    "\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€º\n",
    "Â¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯\\x85âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£\n",
    "âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯\n",
    "â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜\n",
    "\"\"\"\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in SYMBOLS_TO_ISOLATE}\n",
    "remove_dict = {ord(c):f'' for c in SYMBOLS_TO_DELETE}\n",
    "\n",
    "def handle_symbols(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "def handle_contractions(x):\n",
    "    x = treebank_tokenizer.tokenize(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Apostrophe at the start of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    text = handle_symbols(text)\n",
    "    text = handle_contractions(text)\n",
    "    text = fix_quote(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniyal\\AppData\\Local\\Temp\\ipykernel_21804\\1976712352.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_df['target'] =np.where(relevant_df['target'] >=0.5,1,0)\n"
     ]
    }
   ],
   "source": [
    "relevant_df['target'] =np.where(relevant_df['target'] >=0.5,1,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniyal\\AppData\\Local\\Temp\\ipykernel_21804\\2888462913.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_df['comment_text'] = relevant_df['comment_text'].apply(lambda x: preprocess_text(x))\n"
     ]
    }
   ],
   "source": [
    "relevant_df['comment_text'] = relevant_df['comment_text'].apply(lambda x: preprocess_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.70\n",
    "val_size = 0.15\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(relevant_df.sample(frac=1), [int(train_size * len(relevant_df)), int((train_size+test_size) * len(relevant_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263411, 270731, 270732)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(validate),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "val_df = pd.DataFrame(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../model/tokenizer.pkl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m rb:\n\u001b[1;32m----> 2\u001b[0m     tokenizer \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(rb)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open(\"../model/tokenizer.pkl\",\"rb\") as rb:\n",
    "    tokenizer = pickle.load(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out some MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Using cached mlflow-2.2.1-py3-none-any.whl (17.6 MB)\n",
      "Requirement already satisfied: numpy<2 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (1.23.5)\n",
      "Collecting matplotlib<4\n",
      "  Using cached matplotlib-3.7.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "Collecting Jinja2<4,>=3.0\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting shap<1,>=0.40\n",
      "  Using cached shap-0.41.0-cp310-cp310-win_amd64.whl (435 kB)\n",
      "Requirement already satisfied: pytz<2023 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (2022.7.1)\n",
      "Collecting scikit-learn<2\n",
      "  Using cached scikit_learn-1.2.1-cp310-cp310-win_amd64.whl (8.3 MB)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (2.28.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (6.0)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Collecting Flask<3\n",
      "  Using cached Flask-2.2.3-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: waitress<3 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (2.1.2)\n",
      "Collecting alembic<2\n",
      "  Using cached alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (8.1.3)\n",
      "Collecting entrypoints<1\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0\n",
      "  Using cached SQLAlchemy-2.0.4-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0\n",
      "  Using cached importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pandas<3 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (1.5.3)\n",
      "Requirement already satisfied: packaging<24 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (23.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (3.19.6)\n",
      "Requirement already satisfied: querystring-parser<2 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (0.4.3)\n",
      "Collecting scipy<2\n",
      "  Using cached scipy-1.10.1-cp310-cp310-win_amd64.whl (42.5 MB)\n",
      "Collecting cloudpickle<3\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting pyarrow<12,>=4.0.0\n",
      "  Using cached pyarrow-11.0.0-cp310-cp310-win_amd64.whl (20.6 MB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from mlflow) (3.4.1)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Using cached databricks_cli-0.17.4-py3-none-any.whl\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Using cached docker-6.0.1-py3-none-any.whl (147 kB)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from click<9,>=7.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.6.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from docker<7,>=4.0.0->mlflow) (1.26.14)\n",
      "Requirement already satisfied: pywin32>=304 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from docker<7,>=4.0.0->mlflow) (305)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from docker<7,>=4.0.0->mlflow) (1.5.1)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from Flask<3->mlflow) (2.2.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from matplotlib<4->mlflow) (9.4.0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.7-cp310-cp310-win_amd64.whl (162 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from shap<1,>=0.40->mlflow) (0.0.7)\n",
      "Collecting numba\n",
      "  Using cached numba-0.56.4-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: tqdm>4.25.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from shap<1,>=0.40->mlflow) (4.64.1)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-2.0.2-cp310-cp310-win_amd64.whl (192 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (4.5.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Using cached llvmlite-0.39.1-cp310-cp310-win_amd64.whl (23.2 MB)\n",
      "Requirement already satisfied: setuptools in d:\\daniyal\\personal projects\\toxic comments\\toxic_comments\\lib\\site-packages (from numba->shap<1,>=0.40->mlflow) (65.5.0)\n",
      "Installing collected packages: scipy, pyarrow, Mako, llvmlite, kiwisolver, Jinja2, itsdangerous, importlib-metadata, greenlet, gitdb, fonttools, entrypoints, cycler, contourpy, cloudpickle, sqlalchemy, scikit-learn, numba, matplotlib, gitpython, Flask, docker, databricks-cli, shap, alembic, mlflow\n",
      "Successfully installed Flask-2.2.3 Jinja2-3.1.2 Mako-1.2.4 alembic-1.9.4 cloudpickle-2.2.1 contourpy-1.0.7 cycler-0.11.0 databricks-cli-0.17.4 docker-6.0.1 entrypoints-0.4 fonttools-4.38.0 gitdb-4.0.10 gitpython-3.1.31 greenlet-2.0.2 importlib-metadata-6.0.0 itsdangerous-2.1.2 kiwisolver-1.4.4 llvmlite-0.39.1 matplotlib-3.7.1 mlflow-2.2.1 numba-0.56.4 pyarrow-11.0.0 scikit-learn-1.2.1 scipy-1.10.1 shap-0.41.0 sqlalchemy-2.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/04 18:13:04 INFO mlflow.tracking.fluent: Experiment with name 'testing' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:/Daniyal/Personal%20Projects/Toxic%20Comments/notebooks/mlruns/241161762715750035', creation_time=1677935584995, experiment_id='241161762715750035', last_update_time=1677935584995, lifecycle_stage='active', name='testing', tags={}>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"a\", 1)\n",
    "    mlflow.log_metric(\"b\", 2)\n",
    "    \n",
    "    for i in range(10):\n",
    "        mlflow.log_metric(\"loss\",i)\n",
    "        mlflow.log_metric(\"epoch\",i)\n",
    "        \n",
    "    mlflow.log_artifact(\"../model/tokenizer.pkl\",\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Glove embedding table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(tokenizer.word_index) + 1\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 100\n",
    "model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self, embedding_matrix, num_aux_targets):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    embed_size = embedding_matrix.shape[1]\n",
    "    self.embedding = nn.Embedding(max_features, embed_size)\n",
    "    self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "    self.embedding.weight.requires_grad = False\n",
    "    self.embedding_dropout = SpatialDropout(0.3)\n",
    "    self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "    self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "\n",
    "    self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "    self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h_embedding = self.embedding(x)\n",
    "    h_embedding = self.embedding_dropout(h_embedding)\n",
    "\n",
    "    h_lstm1, _ = self.lstm1(h_embedding)\n",
    "    h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "\n",
    "    # global average pooling\n",
    "    avg_pool = torch.mean(h_lstm2, 1)\n",
    "    # global max pooling\n",
    "    max_pool, _ = torch.max(h_lstm2, 1)\n",
    "\n",
    "    h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "    h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "    h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "\n",
    "    hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "    result = self.linear_out(hidden)\n",
    "    aux_result = self.linear_aux_out(hidden)\n",
    "    out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniyal\\AppData\\Local\\Temp\\ipykernel_21804\\3498784665.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "unexpected end of input; is count incorrect or file otherwise damaged?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m word2vec_glove_file \u001b[39m=\u001b[39m get_tmpfile(\u001b[39m\"\u001b[39m\u001b[39mglove.840B.300d.word2vec.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m glove2word2vec(glove_file, word2vec_glove_file)\n\u001b[1;32m----> 7\u001b[0m glove_model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(word2vec_glove_file)\n",
      "File \u001b[1;32md:\\Daniyal\\Personal Projects\\Toxic Comments\\toxic_comments\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32md:\\Daniyal\\Personal Projects\\Toxic Comments\\toxic_comments\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2068\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[0;32m   2070\u001b[0m \u001b[39mif\u001b[39;00m kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(kv):\n\u001b[0;32m   2071\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   2072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2073\u001b[0m         kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(kv),\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32md:\\Daniyal\\Personal Projects\\Toxic Comments\\toxic_comments\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1973\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1971\u001b[0m line \u001b[39m=\u001b[39m fin\u001b[39m.\u001b[39mreadline()\n\u001b[0;32m   1972\u001b[0m \u001b[39mif\u001b[39;00m line \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m-> 1973\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1974\u001b[0m word, weights \u001b[39m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[0;32m   1975\u001b[0m _add_word_to_kv(kv, counts, word, weights, vocab_size)\n",
      "\u001b[1;31mEOFError\u001b[0m: unexpected end of input; is count incorrect or file otherwise damaged?"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_file = datapath('D:\\Daniyal\\Personal Projects\\Toxic Comments\\model\\glove.840B.300d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.840B.300d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_comments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02cecfd0603deda73531c6d8bfb594cadd5ae60430ed950aec9377b20345d8ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
