columnns =  ['target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']

train_size = 0.70
test_size = 0.15
validate_size = 0.15
max_features = 50000
tokenizer_path = "../model/tokenizer.pkl"